{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw7h7RBQXyZM"
      },
      "source": [
        "# In-Context Learning\n",
        "\n",
        "\n",
        "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
        "\n",
        "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
        "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
        "\n",
        "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
        "* a search engine is used to retrieve some relevant information\n",
        "* that information is then provided to the LLM as context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWVwoHjEXyZN"
      },
      "source": [
        "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA7vp0l7XyZO",
        "outputId": "00fd1a16-529e-45f5-a104-4a6b3413c11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, bs4\n",
            "Successfully installed bs4-0.0.2 pypdf-6.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install requests bs4 google-generativeai pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yNQNxV3RXyZO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from IPython.display import Markdown, display\n",
        "from pypdf import PdfReader\n",
        "from datetime import date\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmEr2SVhXyZO"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"AIzaSyA0mXFee40Cyg7lIK4NbOP308-49\"\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0fW7usJXyZO"
      },
      "source": [
        "We select those papers that have been featured in Hugging Face papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EQTSodTkXyZP"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://huggingface.co/papers\"\n",
        "page = requests.get(BASE_URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "h3s = soup.find_all(\"h3\")\n",
        "\n",
        "papers = []\n",
        "\n",
        "for h3 in h3s:\n",
        "    a = h3.find(\"a\")\n",
        "    title = a.text\n",
        "    link = a[\"href\"].replace('/papers', '')\n",
        "\n",
        "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgjdt6R4XyZP"
      },
      "source": [
        "Code to extract text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O35CQVM-XyZP"
      },
      "outputs": [],
      "source": [
        "def extract_paper(url):\n",
        "    html = urlopen(url).read()\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_pdf(url):\n",
        "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
        "    reader = PdfReader(\"pdf_file.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoeAW6xiXyZP"
      },
      "outputs": [],
      "source": [
        "LLM = \"gemini-2.5-flash\"\n",
        "model = genai.GenerativeModel(LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-wsSH_0XyZP"
      },
      "source": [
        "We use Gemini to summarize the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TpVx-aZ3aIJI"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are given the full text of a research paper.\n",
        "\n",
        "Produce a single Markdown table with exactly three columns:\n",
        "\n",
        "| Summary | Strengths | Weaknesses |\n",
        "\n",
        "Guidelines:\n",
        "- The Summary column should contain a concise one-paragraph summary of the paper.\n",
        "- The Strengths and Weaknesses columns should contain short, clear analytical points.\n",
        "- All content must be inside ONE Markdown table.\n",
        "- Do NOT include any text before or after the table.\n",
        "- Use valid Markdown table syntax.\n",
        "- Base your analysis only on the given paper.\n",
        "\n",
        "Paper text:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "      paper_text = extract_pdf(paper[\"url\"])\n",
        "      paper[\"summary\"] = model.generate_content(PROMPT_TEMPLATE + paper_text).text\n",
        "\n",
        "    except:\n",
        "      print(\"Generation failed\")\n",
        "      paper[\"summary\"] = \"Paper not available\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCODAWm0cWqP",
        "outputId": "6479c7ef-ce7f-413a-b164-41fe4f376735"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 15/20 [05:54<01:53, 22.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [07:26<00:00, 22.30s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UupY594PXyZP"
      },
      "source": [
        "We print the results to a html file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WrFPfSxgAiG",
        "outputId": "eeef33d1-418f-4572-99c1-b1b6e76b228d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import markdown"
      ],
      "metadata": {
        "id": "9kXI_RCrgD8M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q2gL32sOXyZP"
      },
      "outputs": [],
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    # Modify the codes to show the table\n",
        "    html_summary = markdown.markdown(\n",
        "        paper[\"summary\"],\n",
        "        extensions=[\"tables\"]\n",
        "    )\n",
        "\n",
        "    page = f\"\"\"\n",
        "    <h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2>\n",
        "    {html_summary}\n",
        "    \"\"\"\n",
        "\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8iBjNmTXyZP"
      },
      "source": [
        "We can also print the results to this notebook as markdown."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the code to show the table\n",
        "for paper in papers:\n",
        "    printmd(\n",
        "        f\"## [{paper['title']}]({paper['url']})\\n\\n\"\n",
        "        f\"{paper['summary']}\\n\\n---\\n\"\n",
        "    )"
      ],
      "metadata": {
        "id": "TlcNBx1IhXwu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00af262c-0de0-4d52-e3e0-74ef6b5efaa2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/pdf/2512.24880)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces Manifold-Constrained Hyper-Connections (mHC), an extension of Hyper-Connections (HC) designed to address HC's training instability, restricted scalability, and memory overhead. mHC projects HC's residual connection matrices onto a specific manifold (doubly stochastic matrices using Sinkhorn-Knopp) to restore the identity mapping property, ensuring signal stability. It also incorporates rigorous infrastructure optimizations like kernel fusion, recomputing, and communication overlapping for efficiency. Empirical results demonstrate mHC's superior stability, scalability, and performance gains in large-scale language model pre-training with marginal overhead. | Restores identity mapping property, significantly improving training stability and mitigating signal explosion/vanishing. Achieves tangible performance improvements and superior scalability compared to both baseline and unconstrained HC. Incorporates rigorous infrastructure optimizations (kernel fusion, recomputing, communication overlapping) to ensure efficiency with minimal overhead. Benefits from strong theoretical properties of doubly stochastic matrices, such as norm preservation and compositional closure. | The Sinkhorn-Knopp algorithm provides an approximate solution, leading to slight deviations from perfect double stochasticity, especially in composite mappings. Introduces additional architectural and implementation complexity compared to standard residual connections. Despite optimizations, there is still a reported 6.7% additional time overhead for an expansion rate of n=4. The current manifold choice (doubly stochastic matrices) might not be universally optimal, and other constraints could be explored. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models](https://arxiv.org/pdf/2512.24618)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| Youtu-LLM is a 1.96B-parameter language model pre-trained from scratch to achieve high computational efficiency and native agentic intelligence. It features a compact MLA architecture with 128k context, a STEM-oriented tokenizer, and a multi-stage \"Commonsense-STEM-Agent\" curriculum. Its scalable agentic mid-training, utilizing diverse trajectory data, cultivates intrinsic planning and reflection. Evaluations show Youtu-LLM sets new state-of-the-art for sub-2B LLMs, outperforming or rivaling larger models on general and agent-specific benchmarks. | Achieves strong native agentic capabilities through systematic pre-training, not just post-hoc methods. Highly computationally efficient (1.96B parameters) with long-context support (128k) for resource-constrained use. Employs a comprehensive multi-stage training curriculum and high-quality, diverse agentic trajectory data. Demonstrates state-of-the-art performance for sub-2B models, often surpassing larger LLMs on agentic tasks. Utilizes robust training methodologies including two-stage SFT and refined RL for stability and performance. | Agentic capabilities still lag behind very large proprietary LLMs due to computational resource limitations. Long reasoning trajectories can lead to increased inference latency, impacting efficiency. Current scope is limited to text-based environments, lacking multimodal perception. Absence of suitable agentic mathematical benchmarks for instruction-tuned models hinders comprehensive evaluation. Potential for performance degradation if domain-specific trajectory data is excessively upsampled, indicating a need for careful data balancing. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/pdf/2512.24873)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces the Agentic Learning Ecosystem (ALE), a comprehensive infrastructure comprising ROLL (RL training), ROCK (sandboxed environments), and iFlow CLI (agent framework) designed to streamline the development and deployment of agentic LLMs for complex, multi-turn tasks. It presents ROME, an open-source agent LLM trained within ALE using a novel Interaction-Perceptive Agentic Policy Optimization (IPA) algorithm and rigorously curated data, achieving strong performance on various agentic benchmarks and demonstrating production readiness. | - Provides a complete, end-to-end open-source ecosystem (ALE) for agentic LLM development and deployment.<br>- Introduces IPA, a novel RL algorithm that improves training stability and efficiency for long-horizon tasks by optimizing over semantic interaction chunks.<br>- Employs a robust, multi-tiered data composition strategy with strong verification and safety alignment.<br>- ROME achieves competitive performance on diverse agentic benchmarks, often outperforming similarly sized models and rivaling larger ones.<br>- Designed for production deployment, demonstrated by ROME's successful integration and real-world validation.<br>- Introduces Terminal Bench Pro, a more rigorous benchmark for terminal agents.<br>- Explicitly addresses and integrates safety-aligned data to mitigate general-security issues in agent behavior. | - ROME's performance on the most challenging benchmark (Terminal Bench Pro) remains limited, indicating significant room for improvement in complex real-world tasks.<br>- The comprehensive ecosystem might present a steep learning curve and high setup complexity for new users.<br>- Heavy reliance on synthesized data, despite verification, could introduce biases or limit real-world generalizability.<br>- The extensive data curation and filtering process, while beneficial, could potentially lead to overfitting on benchmark characteristics.<br>- The computational resources required for training on \"over one million trajectories\" and complex RL might be substantial, limiting accessibility.<br>- The systematic investigation of safety measures is noted as future work, suggesting current safety protocols may not be fully comprehensive or generalized. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction](https://arxiv.org/pdf/2512.25073)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| GaMO (Geometry-aware Multi-view Diffusion Outpainter) is a novel framework for sparse-view 3D reconstruction that reformulates the problem as multi-view outpainting rather than novel view generation. By expanding the field of view of existing input images using a zero-shot multi-view diffusion model conditioned on geometry priors, GaMO effectively mitigates common artifacts like holes, ghosting, and geometric inconsistencies. The method achieves state-of-the-art reconstruction quality on Replica and ScanNet++ datasets with significant speed improvements (25x faster) over previous diffusion-based approaches. | - Effectively mitigates holes, ghosting, and geometric inconsistencies in sparse-view 3D reconstruction. <br> - Achieves state-of-the-art performance across multiple metrics (PSNR, SSIM, LPIPS, FID) on benchmark datasets. <br> - Significantly faster (25x speedup, under 10 minutes) compared to prior diffusion-based methods. <br> - Operates in a zero-shot manner, requiring no fine-tuning of the diffusion backbone. <br> - Preserves geometric consistency by expanding existing views' FOV rather than generating new viewpoints. <br> - Demonstrates strong generalization to diverse scenes, including large-scale 360-degree environments. | - Unable to recover content that is completely occluded from all input views. <br> - Performance is sensitive to the distribution and alignment of input views. <br> - Heavily occluded regions remain challenging for reconstruction. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers](https://arxiv.org/pdf/2512.23380)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| CoLog is a novel framework for operating system log anomaly detection that addresses the limitations of existing unimodal and multimodal approaches. It formulates log anomaly detection as a multimodal sentiment analysis problem, enabling the unified detection of both point and collective anomalies. CoLog achieves this by collaboratively encoding semantic and sequence log modalities using a collaborative transformer architecture with multi-head impressed attention to capture nuanced interactions, and a modality adaptation layer to handle data heterogeneity. The framework demonstrates superior performance, achieving high precision, recall, and F1 scores across multiple benchmark datasets, and its implementation is publicly available. | - Unified framework capable of detecting both point and collective anomalies. <br> - Novel architecture combining collaborative transformers, multi-head impressed attention, and a modality adaptation layer. <br> - Achieves state-of-the-art performance with high accuracy, precision, recall, and F1-scores across diverse benchmark datasets. <br> - Demonstrates strong generalization and robustness to unseen log events and new datasets. <br> - Effectively handles class imbalance using the Tomek link technique. <br> - Open-source implementation promotes reproducibility and further research. <br> - Meaningful fusion of modalities in a shared latent space. | - Evaluated in batch-processing mode; real-time performance regarding latency and computational resources is not fully explored. <br> - Initial interpretation and labeling of complex, human-unreadable OS log entries can be challenging. <br> - Requires continual adaptation and retraining to maintain validity with evolving log structures and system updates. <br> - Inference time is higher compared to some other deep learning models like LSTM or CNN. <br> - Performance may degrade in extreme class imbalance scenarios (e.g., anomaly ratio <1%) or with rapidly evolving log templates. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/pdf/2512.25070)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces a method to train language models for open-ended forecasting by synthesizing a large-scale dataset, OpenForesight, from daily news using an automated, leakage-preventing pipeline. They train the OpenForecaster8B model with reinforcement learning, employing a novel Accuracy + Brier score reward function and retrieval from an offline news corpus. The resulting 8B model achieves competitive performance against much larger proprietary models in accuracy, calibration, and consistency on held-out test sets and external benchmarks, and also shows improved calibration on general LLM benchmarks. All models, code, and data are open-sourced to foster further research. | 1. Novel, automated, and scalable data generation method for open-ended forecasting questions from news. <br> 2. Robust measures to prevent future information leakage during data generation and evaluation. <br> 3. Proposes and validates an effective combined Accuracy + Brier score reward function for RL, improving both accuracy and calibration. <br> 4. OpenForecaster8B (8B parameters) achieves strong performance, matching or outperforming much larger proprietary models. <br> 5. Open-sources all models, code, and the OpenForesight dataset, promoting accessibility and further research. <br> 6. Demonstrates that forecasting training generalizes to improve calibration on popular LLM benchmarks. | 1. Reliance on news for question generation introduces a distributional bias in forecasted events. <br> 2. Acknowledges that late reporting in news can make some events easier to \"predict\" in the dataset. <br> 3. Does not address long-form forecasts, citing grading difficulties. <br> 4. The data generation process, especially for high-quality validation/test sets, can be costly. <br> 5. The primary held-out test set covers a relatively short future time horizon (May-August 2025). |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation](https://arxiv.org/pdf/2512.24551)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| The paper introduces PhyGDPO, a Physics-aware Groupwise Direct Preference Optimization framework designed to enhance physical consistency in text-to-video (T2V) generation. It proposes PhyAugPipe, a VLM-driven pipeline that constructs PhyVidGen-135K, a large dataset rich in physics interactions. PhyGDPO employs a groupwise Plackett–Luce model, a Physics-Guided Rewarding (PGR) scheme leveraging VLM-based physics scores, and a LoRA-Switch Reference (LoRA-SR) for efficient and stable training. Comprehensive experiments demonstrate that PhyGDPO significantly outperforms state-of-the-art methods, including commercial models like Sora2 and Veo3.1, in physical plausibility and human preference. | Addresses a critical limitation in T2V generation by improving physical consistency. <br> Introduces a novel data construction pipeline (PhyAugPipe) to create a large, physics-rich dataset. <br> Formulates a principled groupwise DPO framework for holistic preference learning. <br> Integrates VLM-based physics rewards (PGR) for more effective optimization guidance. <br> Improves training efficiency and stability with the LoRA-Switch Reference (LoRA-SR) mechanism. <br> Achieves strong empirical results, outperforming SOTA models on benchmarks and human evaluation. <br> Focuses on exploring the implicit physics reasoning ability of T2V models. | Relies on an external physics-aware VLM (VideoCon-Physics) for rewarding, potentially inheriting its biases or limitations. <br> The PhyAugPipe data construction pipeline is complex, involving multiple VLM-driven steps, which might be resource-intensive or difficult to reproduce. <br> The Physics-Guided Rewarding scheme introduces several hyperparameters that require careful tuning. <br> The use of an upper bound approximation for the groupwise DPO loss might affect theoretical guarantees or convergence properties. <br> Despite significant relative improvements, absolute scores on challenging \"hard actions\" remain relatively low, indicating persistent difficulties with complex physical phenomena. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/pdf/2512.23343)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This survey paper offers a comprehensive and unified review of memory systems, bridging insights from cognitive neuroscience with LLM-driven autonomous agents. It systematically defines memory across biological and artificial contexts, provides a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle (extraction, updating, retrieval, utilization). Additionally, the paper reviews mainstream benchmarks for evaluating agent memory, explores memory security from both attack and defense perspectives, and envisions future research directions such as multimodal memory systems and agent skill acquisition, aiming to foster cross-disciplinary collaboration and advance robust agent memory systems. | - Offers a comprehensive, unified interdisciplinary perspective on memory from cognitive neuroscience to LLM-driven agents.<br>- Provides a structured comparative analysis of biological and artificial memory taxonomy, storage, and management.<br>- Addresses practical and critical aspects including memory security (attacks and defenses) and evaluation benchmarks.<br>- Identifies and discusses promising future research directions such as multimodal memory and agent skill acquisition. | - As a broad survey, it may lack in-depth technical detail on specific neuroscientific models or AI implementations.<br>- Primarily synthesizes existing knowledge rather than presenting novel research findings in individual sub-areas.<br>- Does not deeply analyze the limitations or propose improvements for existing agent memory benchmarks.<br>- Focuses heavily on LLM-driven agents, potentially limiting coverage of memory systems in other autonomous agent paradigms.<br>- Provides high-level inspiration for bridging the brain-AI gap but lacks concrete, actionable methodologies for direct translation. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [GR-Dexter Technical Report](https://arxiv.org/pdf/2512.24210)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| GR-Dexter is a holistic hardware-model-data framework designed for generalist manipulation on bimanual robots equipped with high degree-of-freedom (DoF) dexterous hands. It introduces the compact 21-DoF ByteDexter V2 hand, an intuitive VR-based bimanual teleoperation system for efficient data collection, and a comprehensive training recipe that integrates teleoperated robot trajectories with large-scale vision-language, cross-embodiment, and human trajectory datasets. This approach enables strong in-domain performance and improved robustness to unseen objects, instructions, and out-of-distribution scenarios in complex, long-horizon manipulation tasks. | - **Advanced Hardware:** Features the compact, 21-DoF ByteDexter V2 anthropomorphic hand with tactile sensors, designed for durability and ease of maintenance. <br> - **Efficient Data Collection:** Utilizes an intuitive VR-based bimanual teleoperation system for collecting high-quality, long-horizon robot trajectories. <br> - **Comprehensive Training Data:** Leverages a diverse \"data pyramid\" including teleoperated robot data, vision-language data, cross-embodiment demonstrations, and human trajectories for robust learning. <br> - **Enhanced Generalization:** Demonstrates improved robustness to unseen objects, novel spatial configurations, and abstract language instructions in real-world tasks. <br> - **Strong Performance:** Achieves high success rates in complex, long-horizon bimanual manipulation and generalizable pick-and-place tasks. <br> - **Holistic Framework:** Integrates hardware, model, and data collection into a cohesive system for dexterous manipulation. | - **Limited Human Data Scale:** Only a few hundred hours of human trajectories were utilized, leaving a significant amount of potential data untapped. <br> - **Suboptimal Hand-Arm Coordination:** The current control system separates hand and arm control, potentially hindering tight coordination in contact-rich tasks. <br> - **Data Collection Constraints:** Large-scale teleoperation data collection is limited by hardware availability and the scarcity of skilled teleoperators. <br> - **Complexity of Data Integration:** Requires careful calibration, standardization, quality control, and retargeting for cross-embodiment and human data, which can be challenging. <br> - **VLM Co-training Overhead:** In some in-distribution settings, co-training with vision-language data alone (without cross-embodiment) can make optimization more challenging compared to teleop-only baselines. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process](https://arxiv.org/pdf/2512.23988)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces RISE (Reasoning behavior Interpretability via Sparse auto-Encoder), an unsupervised framework for discovering and manipulating reasoning behaviors in Large Language Models (LLMs). By training Sparse Auto-Encoders (SAEs) on sentence-level activations from chain-of-thought traces, the method identifies \"reasoning vectors\" that encode distinct behaviors like reflection and backtracking. The framework demonstrates that these vectors are disentangled, occupy separable regions in activation space, and can be used to controllably amplify or suppress specific reasoning styles during inference without retraining. RISE also uncovers novel behaviors, such as confidence, and structural properties like response length, ultimately showing potential for both interpreting and steering LLM reasoning processes to improve accuracy and efficiency. | Unsupervised discovery of reasoning behaviors overcomes limitations of human-defined, supervised concepts. <br> Disentangles interpretable behaviors (e.g., reflection, backtracking) into linear directions in activation space. <br> Enables causal intervention to amplify or suppress specific behaviors during inference without model retraining. <br> Demonstrates the ability to discover novel behaviors beyond human supervision, exemplified by confidence-related vectors. <br> Learned reasoning vectors generalize across different tasks and models. <br> Provides insights into structural properties like response length organization within the latent space. <br> Shows practical application in improving reasoning accuracy and reducing token cost through targeted steering. <br> Supported by theoretical justification and empirical validation of underlying assumptions. | Relies on LLM-as-a-judge (GPT-5) for human-supervised validation of SAE geometry, which introduces potential for LLM biases despite consistency checks. <br> The \"novel behaviors\" demonstrated (e.g., confidence) are somewhat established concepts in LLM analysis, even if difficult to define at a word level. <br> Intervening with confidence vectors sometimes led to a slight performance drop, suggesting potential trade-offs not fully explored. <br> The intervention method, projecting out components along a direction, might be a blunt instrument with unexamined side effects. <br> The findings are tied to the specific properties of SAEs; applicability to other interpretability methods might vary. <br> Primarily focuses on math/logic tasks for training and evaluation, which might limit the diversity of reasoning behaviors discovered or the generalizability to broader domains. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/pdf/2512.23851)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces a neural network architecture for compressing long video histories into short contexts, specifically designed for autoregressive video generation. It employs an explicit pretraining objective to ensure high-fidelity retrieval of individual frames at arbitrary temporal positions, thereby preserving high-frequency details. The pretrained compression model serves as an efficient memory encoder, which, when fine-tuned with autoregressive video diffusion models, enables long-range consistency and reduces overall training costs by balancing context length and quality. | Effectively addresses the critical trade-off between context length and quality for long video generation. <br> Novel pretraining objective for high-fidelity frame retrieval at arbitrary temporal positions. <br> Achieves significant memory compression (e.g., 20s video to ~5k context) with good perceptual quality. <br> Improves long-range temporal consistency in generated videos. <br> Reduces training costs for autoregressive video models through pretraining. <br> Provides a practical and adaptable framework, compatible with existing diffusion models and extensible with other techniques. <br> Offers white-box feedback for better understanding and managing encoding components. <br> Supported by extensive quantitative and qualitative evaluations, including human studies. | Enhanced configurations (cross-attention, multiple encoders) increase computational cost and context length. <br> Error accumulation/drifting remains a challenge for very long, continuous single-shot videos, despite efforts. <br> Requires a large dataset (millions of videos) for effective pretraining, which might be a barrier for some. <br> The architecture, while efficient, involves multiple complex components. <br> Limited in-depth exploration of alternative compression model architectures beyond the baseline. <br> The concept of \"perceptually preserved appearances\" can be subjective despite quantitative metrics. <br> Tailored specifically for Diffusion Transformers (DiTs), potentially limiting broader applicability without modification. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time](https://arxiv.org/pdf/2512.25075)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| SpaceTimePilot is a novel video diffusion model that achieves disentangled control over camera viewpoint and motion sequences from a single monocular video. It introduces an \"animation time\" embedding and an improved source-aware camera conditioning mechanism. To facilitate training, the model leverages a temporal-warping scheme for existing multi-view datasets and a novel synthetic dataset, Cam×Time, which provides dense spatiotemporal supervision. This framework enables continuous and arbitrary exploration across space and time, supporting effects like slow motion, reverse playback, and bullet time with precise camera control, outperforming prior methods. | - First model to achieve joint and disentangled spatial and temporal control in video diffusion. <br> - Enables diverse temporal effects (slow motion, reverse, bullet time) with precise camera control. <br> - Introduces innovative temporal-warping scheme for data augmentation from existing datasets. <br> - Proposes Cam×Time, a novel synthetic dataset providing dense spatiotemporal supervision for disentanglement. <br> - Features an improved source-aware camera conditioning mechanism, enhancing camera control accuracy. <br> - Supports autoregressive generation for extended, coherent video sequences beyond initial frame limits. <br> - Demonstrates superior quantitative and qualitative performance against state-of-the-art baselines. | - Reliance on synthetic data (Cam×Time) and augmented real data, which may limit generalization to highly complex or novel real-world scenarios. <br> - The \"animation time\" mechanism primarily re-times existing motion rather than generating entirely new dynamic behaviors. <br> - Potential for accumulated errors or drift in very long autoregressively generated sequences, a common challenge for such methods. <br> - Computational intensity and resource requirements are likely high, typical for advanced video diffusion models, though not explicitly detailed. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Guiding a Diffusion Transformer with the Internal Dynamics of Itself](https://arxiv.org/pdf/2512.24176)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| The paper introduces Internal Guidance (IG), a novel strategy to enhance diffusion transformer models by leveraging their internal dynamics. During training, IG applies auxiliary supervision to intermediate layers. During sampling, it extrapolates between the intermediate and deep layer outputs to guide the generation process, effectively creating a \"bad version\" guidance without requiring extra training or sampling steps. This plug-and-play method significantly improves generation quality and training efficiency, maintaining diversity, and is compatible with existing techniques like Classifier-Free Guidance (CFG). Experiments show IG achieves state-of-the-art FID scores, such as 1.19 on ImageNet 256x256 with LightningDiT-XL/1, and demonstrates improved scalability and faster convergence compared to baselines. | - Simple and effective \"plug-and-play\" guidance strategy.<br>- Achieves state-of-the-art generation quality (e.g., FID 1.19).<br>- Significantly improves training efficiency and accelerates convergence.<br>- Maintains generation diversity, unlike high-coefficient CFG.<br>- Requires no additional sampling steps or extra model training.<br>- Compatible and complementary with Classifier-Free Guidance (CFG).<br>- Scales well, with benefits increasing for larger models.<br>- Alleviates vanishing gradients in deep networks.<br>- Introduces negligible computational overhead during sampling. | - Optimal IG coefficient and guidance interval settings are context-dependent (e.g., with/without CFG) and require careful tuning.<br>- Applying auxiliary supervision in later layers or multiple layers can hinder convergence.<br>- High IG coefficients on insufficiently trained models can introduce new outliers.<br>- Potential for training instability in certain base model setups (e.g., LightningDiT with specific optimizers), requiring workarounds.<br>- The fixed nature of the \"bad version\" (intermediate layer) might limit flexibility compared to explicitly designed degradation strategies for specific use cases. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/pdf/2512.22905)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| JavisGPT is presented as the first unified multimodal large language model (MLLM) for joint audio-video (JAV) comprehension and generation. It employs an encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video alignment and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. The model is trained via a three-stage pipeline and leverages JavisInst-Omni, a new 200K GPT-4o-curated instruction dataset, achieving state-of-the-art performance in JAV comprehension and generation, especially in complex and temporally synchronized contexts. | First unified MLLM for sounding-video comprehension and generation. <br> Novel SyncFusion module effectively captures spatio-temporal audio-video synchrony. <br> Robust three-stage training pipeline (multimodal pretraining, AV fine-tuning, instruction-tuning). <br> Introduces JavisInst-Omni, a large-scale, high-quality instruction dataset for diverse JAV tasks. <br> Achieves state-of-the-art performance on JAV comprehension and generation benchmarks. <br> Demonstrates superior performance in complex, temporally synchronized settings and interleaved human-like conversations. <br> Efficient SyncFusion reduces token count and inference latency compared to other fusion methods. <br> ST-Prior queries effectively enhance audio-visual synchrony in generated content. <br> Joint training of comprehension and generation tasks shows mutual enhancement. | The encoder-LLM-decoder architecture has misaligned training objectives (NTP vs. diffusion) and asymmetric input-output modeling, limiting full unification. <br> Scalability is not fully explored, currently built on a 7B LLM and limited datasets. <br> Lacks advanced post-training techniques like reinforcement learning for more complex reasoning and further quality improvements. <br> Potential for misuse in generating misleading or harmful deepfakes and spreading misinformation. <br> Risks of reproducing biased or private content if trained on sensitive data. <br> The LLM+DiT paradigm, while effective, might not be the \"ultimate\" unified framework compared to end-to-end autoregressive models. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers](https://arxiv.org/pdf/2512.22564)\n\nPaper not available\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts](https://arxiv.org/pdf/2512.24885)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| The paper introduces BEDA (Belief Estimation for Dialogue Acts), a framework that bridges belief estimation and utterance generation in strategic dialogue by operationalizing dialogue acts as probabilistic constraints. BEDA formalizes two core dialogue acts, Adversarial and Alignment, and consists of a World Set, a Belief Estimator, and a Conditional Generator. Evaluated across competitive (Conditional Keeper–Burglar), cooperative (Mutual Friends), and negotiation (CaSiNo) settings, BEDA consistently outperforms strong baselines, demonstrating significant improvements in success rates and negotiation outcomes by effectively using inferred beliefs to constrain generation. | 1. **Novel Framework:** Bridges the gap between belief estimation and dialogue act generation by using beliefs as probabilistic constraints. <br> 2. **Strong Theoretical Foundation:** Mathematically defines Adversarial and Alignment Dialogue Acts based on rigorous belief and common knowledge formulations. <br> 3. **Empirical Superiority:** Consistently outperforms strong baselines (e.g., CoT, Self-Reflect, MindDial) across diverse dialogue tasks and various LLM backbones (GPT-3.5, GPT-4, LLaMA, Qwen). <br> 4. **Generalizability:** Effective in competitive, cooperative, and negotiation scenarios, demonstrating robustness. <br> 5. **Efficiency Improvements:** Achieves higher success rates with fewer turns and tokens in cooperative tasks (Mutual Friends). <br> 6. **Modularity:** Separates belief estimation (using a lightweight encoder) from LLM generation, reducing computational costs while maintaining performance. <br> 7. **Mitigates Hallucinations:** Reduces common LLM hallucinations like friend-list comparison and looping dialogues by introducing belief-state constraints. | 1. **Fixed World Set:** The current framework relies on a predefined World Set, limiting its adaptability to dynamic or evolving dialogue contexts. <br> 2. **Coarse Dialogue Act Granularity:** Only two broad dialogue act categories (Adversarial, Alignment) are formalized, potentially overlooking finer-grained strategic nuances (e.g., Concession, Hedging). <br> 3. **Lower Accuracy on Complex Tasks:** Belief estimation accuracy is notably lower on the CaSiNo negotiation task compared to simpler synthetic datasets, suggesting challenges with multi-class belief structures. <br> 4. **LLM Backbone Sensitivity:** Weaker LLMs (e.g., LLaMA without fine-tuning) struggle significantly with certain tasks (like Mutual Friends), leading to poor performance and hallucinations, indicating a reliance on strong base models. <br> 5. **Format Errors:** LLMs still exhibit format errors in generated outputs, requiring manual exclusion of invalid responses. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems](https://arxiv.org/pdf/2512.24385)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper provides a comprehensive roadmap for multi-modal data pre-training aimed at achieving \"Spatial Intelligence\" in autonomous systems. It categorizes pre-training paradigms from single-modality to unified frameworks, analyzes platform-specific datasets across vehicles, drones, and other robots, and explores applications like 3D object detection, semantic occupancy prediction, and open-world perception/planning. The work emphasizes the pivotal role of foundation models, generative world models, and Vision-Language-Action (VLA) architectures, while also identifying critical challenges and future research directions for robust real-world deployment. | - Comprehensive and well-structured taxonomy of pre-training methods, datasets, and applications.<br>- Strong emphasis on the role of foundation models and generative objectives for holistic scene understanding.<br>- Highlights effective strategies for data efficiency, such as cross-modal distillation and self-supervised learning.<br>- Provides a clear, forward-looking roadmap with critical challenges and promising future research directions.<br>- Covers diverse autonomous platforms, showcasing broad applicability of the concepts.<br>- Empirical analysis supports the efficacy of discussed pre-training paradigms on key benchmarks. | - Primarily a high-level survey, lacking in-depth technical details of individual algorithms.<br>- The rapidly evolving nature of the field means some \"future directions\" may quickly become current or outdated.<br>- Limited discussion on the ethical, societal, or regulatory implications of advanced autonomous systems.<br>- While touching on planning, the focus remains heavily on perception aspects of spatial intelligence.<br>- Does not offer concrete solutions or detailed research avenues for real-time inference challenges of large foundation models. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking](https://arxiv.org/pdf/2512.24297)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces FIGR, a novel framework that enhances complex reasoning in large language models by integrating \"active visual thinking\" through end-to-end reinforcement learning. FIGR generates executable code to construct and iteratively refine visual representations (figures) during multi-turn problem-solving, providing dynamic and geometrically consistent feedback. An adaptive reward mechanism guides the model to selectively invoke visual reasoning, leading to significant performance improvements on challenging mathematical benchmarks like AIME and BeyondAIME, by enabling more stable and coherent reasoning over global structural properties. | Generates geometrically consistent and interpretable visual feedback via executable code. Adaptive reward mechanism promotes selective and efficient use of visual thinking. End-to-end reinforcement learning enables autonomous learning of visual invocation strategies without supervised fine-tuning. Achieves substantial performance gains on complex mathematical reasoning tasks, outperforming text-only and other multimodal baselines. Addresses limitations of noisy image generation and predefined tool operations in prior multimodal models. | Relies on a separate LLM for \"suitability\" classification, introducing an external dependency. Computational cost of reinforcement learning with multi-turn rollouts and code execution can be high. Dependency on a sandboxed code interpreter introduces potential failure points for code execution. Generalizability of code generation for highly novel or abstract visual representations might be challenging. Primarily demonstrated on mathematical reasoning, with broader applicability implied but not extensively shown. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Factorized Learning for Temporally Grounded Video-Language Models](https://arxiv.org/pdf/2512.24097)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| This paper introduces D2VLM, a novel framework for temporally grounded video-language models that addresses limitations in existing approaches by factorizing the learning of temporal grounding and textual response. D2VLM adopts a \"grounding then answering with evidence referencing\" paradigm, utilizing \"evidence tokens\" to explicitly capture event-level visual semantics. It also proposes Factorized Preference Optimization (FPO) and a synthetic dataset with factorized perturbations to enhance learning for both tasks. Experiments demonstrate D2VLM's superior performance across various video understanding benchmarks. | <ul><li>Decouples temporal grounding and textual response, addressing a key limitation of coupled learning objectives.</li><li>Introduces novel \"evidence tokens\" that capture event-level visual semantics, providing rich context for text generation.</li><li>Proposes Factorized Preference Optimization (FPO) to explicitly optimize for both temporal grounding and textual response.</li><li>Develops a factorized synthetic dataset to enable controlled and scalable preference learning for explicit temporal grounding.</li><li>Achieves state-of-the-art performance across various temporal grounding and dense captioning tasks, often with a smaller model size.</li><li>Incorporates an explicit consistency constraint between grounding and answering stages, enhancing logical coherence.</li></ul> | <ul><li>Synthetic data generation primarily focuses on negative samples, limiting diversity in positive preference data.</li><li>Performance on some tasks, like episodic memory and YouCook2 dense video captioning, still has significant room for improvement.</li><li>General video QA performance, while improved, is noted to lag behind SOTA due to lack of large-scale generic pretraining and smaller model size.</li><li>The framework's multi-component nature (D2VLM, evidence tokens, FPO, synthetic data) might increase implementation complexity.</li><li>Relies on an off-the-shelf LLM for textual perturbations, which could introduce external biases.</li></ul> |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/pdf/2512.22280)\n\n| Summary | Strengths | Weaknesses |\n|---|---|---|\n| Valori is a deterministic AI memory substrate designed to eliminate non-determinism in vector embedding storage and retrieval, which typically arises from floating-point arithmetic differences across hardware. It achieves this by replacing floating-point operations with fixed-point arithmetic (Q16.16) and modeling memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across diverse platforms, enabling reproducible, auditable, and trustworthy AI systems, particularly for safety-critical and regulated applications. | Guarantees bit-identical memory states, snapshots, and search results across different hardware architectures (x86, ARM, RISC-V, WASM). <br> Enables full replayability and auditability for AI systems, crucial for regulated industries and safety-critical applications. <br> Achieves high semantic fidelity (99.8% Recall@10) despite using fixed-point quantization, demonstrating practical applicability. <br> Implements deterministic indexing structures by removing stochasticity from HNSW graph construction. <br> Offers configurable precision layers as a memory contract, allowing trade-offs between performance and precision without sacrificing determinism. <br> Open-source reference implementation is available. | Software-based fixed-point arithmetic can introduce performance overhead compared to hardware-accelerated floating-point operations. <br> Q16.16 fixed-point format has a limited dynamic range, potentially leading to quantization errors for extreme vector component values. <br> Valori's determinism boundary is at the memory layer; it does not address non-determinism originating from the neural network inference (embedding generation) itself. <br> The evaluation of semantic fidelity is based on a specific model and dataset, and universal recall preservation across all models/distributions is not claimed. <br> Some performance optimizations (e.g., SIMD integer instructions) and higher precision formats are noted as future work, implying current limitations. |\n\n---\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}